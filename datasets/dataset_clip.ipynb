{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train set: 907\n",
      "number of validation set: 0\n",
      "number of test set: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train  val  test\n",
       "0    907    0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-05-4244-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-05-4249-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-05-4250-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-05-4382-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-05-4384-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>TCGA-O2-A52S-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>TCGA-O2-A52V-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>TCGA-O2-A52W-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>TCGA-O2-A5IB-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>TCGA-XC-AA0X-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               train  val test\n",
       "0    TCGA-05-4244-01  NaN  NaN\n",
       "1    TCGA-05-4249-01  NaN  NaN\n",
       "2    TCGA-05-4250-01  NaN  NaN\n",
       "3    TCGA-05-4382-01  NaN  NaN\n",
       "4    TCGA-05-4384-01  NaN  NaN\n",
       "..               ...  ...  ...\n",
       "902  TCGA-O2-A52S-01  NaN  NaN\n",
       "903  TCGA-O2-A52V-01  NaN  NaN\n",
       "904  TCGA-O2-A52W-01  NaN  NaN\n",
       "905  TCGA-O2-A5IB-01  NaN  NaN\n",
       "906  TCGA-XC-AA0X-01  NaN  NaN\n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-05-4244-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-05-4249-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-05-4250-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-05-4382-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-05-4384-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-O2-A52S-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-O2-A52V-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-O2-A52W-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-O2-A5IB-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-XC-AA0X-01</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 train    val   test\n",
       "TCGA-05-4244-01   True  False  False\n",
       "TCGA-05-4249-01   True  False  False\n",
       "TCGA-05-4250-01   True  False  False\n",
       "TCGA-05-4382-01   True  False  False\n",
       "TCGA-05-4384-01   True  False  False\n",
       "...                ...    ...    ...\n",
       "TCGA-O2-A52S-01   True  False  False\n",
       "TCGA-O2-A52V-01   True  False  False\n",
       "TCGA-O2-A52W-01   True  False  False\n",
       "TCGA-O2-A5IB-01   True  False  False\n",
       "TCGA-XC-AA0X-01   True  False  False\n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def save_splits(split_datasets, column_keys, filename, boolean_style=False):\n",
    "    ##########################################################################################\n",
    "    splits = [split_datasets[i]['case_id'] for i in range(len(split_datasets))]\n",
    "    # splits = [split_datasets[i]['case_id']+'/'+split_datasets[i]['slide_id'] for i in range(len(split_datasets))]\n",
    "    if not boolean_style:\n",
    "        df = pd.concat(splits, ignore_index=True, axis=1)\n",
    "        df.columns = column_keys\n",
    "    else:\n",
    "        df = pd.concat(splits, ignore_index = True, axis=0)\n",
    "        index = df.values.tolist()\n",
    "       \n",
    "       \n",
    "        one_hot = np.eye(len(split_datasets)).astype(bool)\n",
    "        bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)\n",
    "        df = pd.DataFrame(bool_array, index=index, columns = ['train', 'val', 'test'])\n",
    "\n",
    "    display(df)\n",
    "    # df.to_csv(filename)\n",
    "\n",
    "\n",
    "class Split_Gene_Clip_Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "        wsi_csv_path = '/shared/j.jang/pathai/CLAM/dataset_csv/TCGA-lung-LUAD+LUSC-TMB-pan_cancer-323.csv',\n",
    "        genomics_csv_path = '/shared/js.yun/data/CLAM_data/genomics_data/TCGA-lung-LUAD+LUSC-selected_2847_zscore.csv',        \n",
    "        seed = 1,\n",
    "        n_splits=10,\n",
    "        val_frac=0, \n",
    "        test_frac=0\n",
    "        ):\n",
    "        self.seed = seed\n",
    "        self.n_splits = n_splits\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        self.slide_data = pd.read_csv(wsi_csv_path)[['case_id', 'slide_id', 'label', 'Subtype', 'Mutation Count', 'TMB (nonsynonymous)']]\n",
    "        self.genomics_data = pd.read_csv(genomics_csv_path)\n",
    "        \n",
    "        self.num_data = self.genomics_data.shape[1]                         # 907명\n",
    "        self.num_val = np.round(self.num_data * val_frac).astype(int)       # 91\n",
    "        self.num_test = np.round(self.num_data * test_frac).astype(int)     # 181\n",
    "        self.num_train = self.num_data-self.num_val-self.num_test           # 635\n",
    "        \n",
    "        print(f'number of train set: {self.num_train}')                 # 635\n",
    "        print(f'number of validation set: {self.num_val}')              # 91\n",
    "        print(f'number of test set: {self.num_test}')                   # 181\n",
    "        # df로 저장\n",
    "        columns = ['train', 'val', 'test']\n",
    "        num_splits = [self.num_train, self.num_val, self.num_test]\n",
    "        count_dataset = pd.DataFrame([num_splits], columns=columns)\n",
    "        display(count_dataset)\n",
    "\n",
    "    def create_splits_index(self):        \n",
    "        for i in range(self.n_splits):\n",
    "            all_indices = np.arange(self.num_data).astype(int)\n",
    "            val_index = np.random.choice(all_indices, self.num_val, replace = False) \n",
    "            remaining_ids = np.setdiff1d(all_indices, val_index)\n",
    "            test_index = np.random.choice(remaining_ids, self.num_test, replace = False) \n",
    "            train_index = np.setdiff1d(remaining_ids, test_index)\n",
    "\n",
    "            assert len(train_index)+len(val_index)+len(test_index) > 0\n",
    "            assert len(np.intersect1d(train_index, test_index)) == 0\n",
    "            assert len(np.intersect1d(train_index, val_index)) == 0\n",
    "            assert len(np.intersect1d(val_index, test_index)) == 0\n",
    "\n",
    "            yield train_index, val_index, test_index\n",
    "\n",
    "    def create_split_file_name(self, train_index, val_index, test_index):\n",
    "        train_data = self.genomics_data.columns[train_index]\n",
    "        val_data = self.genomics_data.columns[val_index]\n",
    "        test_data = self.genomics_data.columns[test_index]\n",
    "\n",
    "        train_data = pd.DataFrame(train_data, columns=['case_id'])\n",
    "        val_data = pd.DataFrame(val_data, columns=['case_id'])\n",
    "        test_data = pd.DataFrame(test_data, columns=['case_id'])\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "split_dataset = Split_Gene_Clip_Dataset()\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    train_index, val_index, test_index = next(split_dataset.create_splits_index())\n",
    "    splits = split_dataset.create_split_file_name(train_index, val_index, test_index)\n",
    "    # print(splits[0])\n",
    "    save_splits(splits, ['train', 'val', 'test'], 'splits_{}.csv'.format(i))\n",
    "    save_splits(splits, ['train', 'val', 'test'], 'splits_{}_bool.csv'.format(i), boolean_style=True)\n",
    "    \n",
    "# split_dir=(f'{args.split_dir}TCGA-lung-label_col_{args.label_column}_sub_{\",\".join(args.target_subtype)}'\n",
    "#                 f'-TMB-high-ratio-{args.tmb_high_ratio:.2f}-splits_{args.k}-seed{args.seed}/{args.task}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2847,) torch.Size([44, 256, 768])\n",
      "(2847,) torch.Size([89, 256, 768])\n",
      "(2847,) torch.Size([36, 256, 768])\n",
      "(2847,) torch.Size([130, 256, 768])\n",
      "(2847,) torch.Size([155, 256, 768])\n",
      "(2847,) torch.Size([310, 256, 768])\n",
      "(2847,) torch.Size([168, 256, 768])\n",
      "(2847,) torch.Size([59, 256, 768])\n",
      "(2847,) torch.Size([28, 256, 768])\n",
      "(2847,) torch.Size([38, 256, 768])\n",
      "(2847,) torch.Size([84, 256, 768])\n",
      "(2847,) torch.Size([55, 256, 768])\n",
      "(2847,) torch.Size([335, 256, 768])\n",
      "(2847,) torch.Size([303, 256, 768])\n",
      "(2847,) torch.Size([243, 256, 768])\n",
      "(2847,) torch.Size([76, 256, 768])\n",
      "(2847,) torch.Size([64, 256, 768])\n",
      "(2847,) torch.Size([62, 256, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-6959c2dded60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClip_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenomics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenomics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-6959c2dded60>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mslide_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslide_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slide_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pt_files'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'{}/{}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.svs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 종성님 CLAM - 이 방식으로 해야됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenomics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'data/{key}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_untyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;31m# TODO: Once we decide to break serialization FC, we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "class Clip_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "        split_csv_path = '/shared/js.yun/data/CLAM_data/clip_data/TCGA-lung-splits_5-frac_1_0_0-seed0/splits_0.csv',\n",
    "        genomics_csv_path = '/shared/js.yun/data/CLAM_data/genomics_data/TCGA-lung-LUAD+LUSC-selected_2847_zscore.csv',        \n",
    "        wsi_csv_path = '/shared/j.jang/pathai/CLAM/dataset_csv/TCGA-lung-LUAD+LUSC-TMB-pan_cancer-323.csv',\n",
    "        wsi_feature_dir = '/shared/j.jang/pathai/data/TCGA-lung-x256-features-dino-from-pretrained-vitb-img224/',\n",
    "        split_key = 'train',\n",
    "        seed = 1,\n",
    "        ):\n",
    "        self.wsi_feature_dir = wsi_feature_dir\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # genomics dataset\n",
    "        self.selected_columns = set(pd.read_csv(split_csv_path)[split_key])                      # training set에 있는 환자 set\n",
    "        genomics_data = pd.read_csv(genomics_csv_path)\n",
    "        self.genomics_data = genomics_data.loc[:, genomics_data.columns.isin(self.selected_columns)]\n",
    "        self.length = len(self.selected_columns)\n",
    "\n",
    "        # WSI dataset\n",
    "        slide_data = pd.read_csv(wsi_csv_path)[['case_id', 'slide_id']]\n",
    "        slide_data['patient'] = slide_data['slide_id'].str.split('-').apply(lambda x: '-'.join(x[:3] + [x[3][:2]]))\n",
    "        self.slide_data = slide_data[slide_data['patient'].isin(self.selected_columns)]\n",
    "\n",
    "        # for i in range(1000):\n",
    "        #     indices = slide_data.index[slide_data['patient'] == list(selected_columns)[i]].tolist()\n",
    "        #     if len(indices) > 1:\n",
    "        #         print(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # genomics data 불러옴\n",
    "        genomics = self.genomics_data.iloc[:,idx].to_numpy()\n",
    "\n",
    "        indices = self.slide_data.index[self.slide_data['patient'] == list(self.selected_columns)[idx]].tolist()\n",
    "        if len(indices) > 1:\n",
    "            index = random.choice(indices)\n",
    "        else:\n",
    "            index = indices[0]\n",
    "        case_id = self.slide_data['case_id'][index]\n",
    "        slide_id = self.slide_data['slide_id'][index]\n",
    "        full_path = os.path.join(self.wsi_feature_dir, 'pt_files','{}/{}.pt'.format(case_id, slide_id)).replace('.svs', '') # 종성님 CLAM - 이 방식으로 해야됨\n",
    "        features = torch.load(full_path)\n",
    "\n",
    "        return genomics, features\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Clip_dataset(split_key='train')\n",
    "\n",
    "for genomics, features in train_dataset:\n",
    "    print(genomics.shape, features.size())\n",
    "\n",
    "\n",
    "print('exit')\n",
    "sys.exit(0)\n",
    "\n",
    "class Multi_Task_Dataset(Dataset):\n",
    "    '''\n",
    "    230926\n",
    "        output으로 subtype이랑 tmb label이랑 같이 출력\n",
    "        dataset에서 split까지 나누도록\n",
    "        regression도 가능하도록\n",
    "        LUSC, LUAD 중 하나만도 가능하도록\n",
    "\n",
    "    Args:\n",
    "        one_subtype: Setting 'LUSC' or 'LUAD' will create a dataset with a single subtype.\n",
    "        label_column: The label column to use. [label, TMB (nonsynonymous)]\n",
    "                      Default: label (TMB_low, TMB_high) -> TMB (nonsynonymous)로 변경하여 돌려볼 것\n",
    "    '''\n",
    "    def __init__(\n",
    "            self, \n",
    "            csv_path, \n",
    "            split_path, \n",
    "            data_dir,\n",
    "            shuffle,        # 여기서 안하고 loader하면 됨\n",
    "            seed, \n",
    "            print_info,\n",
    "            label_dict, \n",
    "            label_dict2, \n",
    "            use_h5=True, \n",
    "            target_subtype=['LUSC', 'LUAD'],\n",
    "            label_column='TMB (nonsynonymous)',\n",
    "            tmb_threshold = [0.5,0.5],\n",
    "            regression = False,\n",
    "            split_key = 'train',\n",
    "            balance = [1,0,1],\n",
    "            genomics = None\n",
    "    ):\n",
    "        \n",
    "        self.slide_data = pd.read_csv(csv_path)[['case_id', 'slide_id', 'label', 'Subtype', 'Mutation Count', 'TMB (nonsynonymous)']]\n",
    "        self.slide_data.reset_index(drop=True, inplace=True)\n",
    "        # target_subtype 리스트에 있는 데이터만 필터링\n",
    "        self.slide_data = self.slide_data[self.slide_data['Subtype'].isin(target_subtype)]\n",
    "        self.split_path = split_path        # target_subtype split 데이터만\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dict = label_dict        # tmb label\n",
    "        self.label_dict2 = label_dict2      # subtype label\n",
    "        self.use_h5 = use_h5\n",
    "        self.num_classes = len(label_dict)\n",
    "        self.seed = seed\n",
    "        self.label_column = label_column\n",
    "        self.regression = regression\n",
    "        self.split_key = split_key\n",
    "        self.balance = balance\n",
    "\n",
    "        if genomics:\n",
    "            self.genomics_data = pd.read_csv(genomics)\n",
    "\n",
    "        # self.slide_data 중에서 split_key(train, val, test 중 하나)만 고름\n",
    "        all_splits = pd.read_csv(split_path, dtype=self.slide_data['slide_id'].dtype)  # Without \"dtype=self.slide_data['slide_id'].dtype\", read_csv() will convert all-number columns to a numerical type. Even if we convert numerical columns back to objects later, we may lose zero-padding in the process; the columns must be correctly read in from the get-go. When we compare the individual train/val/test columns to self.slide_data['slide_id'] in the get_split_from_df() method, we cannot compare objects (strings) to numbers or even to incorrectly zero-padded objects/strings. An example of this breaking is shown in https://github.com/andrew-weisman/clam_analysis/tree/main/datatype_comparison_bug-2021-12-01.\n",
    "        split = all_splits[split_key]\n",
    "        split = split.dropna().reset_index(drop=True)\n",
    "        mask = (self.slide_data['case_id']+'/'+self.slide_data['slide_id']).isin(split.tolist())      # clam  \n",
    "        self.slide_data = self.slide_data[mask].reset_index(drop=True)\n",
    "        self.length = len(self.slide_data)\n",
    "        # exit()\n",
    "\n",
    "        # regression이 아니라면 slide_data[label_column]을 0,1로 변경\n",
    "        if not regression and tmb_threshold is not None:\n",
    "            self.tmb_threshold = tmb_threshold\n",
    "            # 각 target_subtype에 대해 label_col 찾아서 label_col 값을 0,1 label로 바꿈. tmb_low: 0, tmb_high: 1 \n",
    "            for subtype, thr in zip(target_subtype, self.tmb_threshold):\n",
    "                mask = self.slide_data['Subtype'] == subtype\n",
    "                self.slide_data.loc[mask, label_column] = self.slide_data.loc[mask, label_column].apply(lambda x: label_dict['TMB_high'] if x >= thr else label_dict['TMB_low'])\n",
    "            self.slide_data[label_column] = self.slide_data[label_column].astype(int) \n",
    "        # Subtype column을 string에서 int로 변경\n",
    "        self.slide_data['Subtype'] = self.slide_data['Subtype'].map(self.label_dict2)\n",
    "\n",
    "        # label 변환을 __get_item__에서 할거라 사용 불가\n",
    "        self.cls_ids_prep()\n",
    "        if print_info:\n",
    "            self.summarize()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case_id = self.slide_data['case_id'][idx]\n",
    "        slide_id = self.slide_data['slide_id'][idx]\n",
    "        label = self.slide_data[self.label_column][idx]\n",
    "        label2 = self.slide_data['Subtype'][idx]\n",
    "        if type(self.data_dir) == dict:\n",
    "            source = self.slide_data['source'][idx]\n",
    "            data_dir = self.data_dir[source]\n",
    "        else:\n",
    "            data_dir = self.data_dir\n",
    "\n",
    "        if not self.use_h5:\n",
    "            if self.data_dir:       # 이게 없을 수가 있나??\n",
    "                # full_path = os.path.join(data_dir, 'pt_files', '{}.pt'.format(slide_id))\t# 이게 원래 CLAM 인듯?\n",
    "                full_path = os.path.join(data_dir, 'pt_files','{}/{}.pt'.format(case_id, slide_id)).replace('.svs', '') # 종성님 CLAM - 이 방식으로 해야됨\n",
    "                features = torch.load(full_path)\n",
    "                # print(label, label2)\n",
    "                return features, label, label2\n",
    "            \n",
    "            else:\n",
    "                return slide_id, label\n",
    "\n",
    "        else:\t# patch coordinates까지 포함되어 있음. coordinate 정보까지 이용하면 도움될 것 같은데 사용 안하고 있음\n",
    "            full_path = os.path.join(data_dir,'h5_files','{}.h5'.format(slide_id))\n",
    "            with h5py.File(full_path,'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "\n",
    "            features = torch.from_numpy(features)\n",
    "            return features, label, coords\n",
    "\n",
    "    def cls_ids_prep(self):\n",
    "        # store ids corresponding each class at the slide level\n",
    "        self.slide_subtype_cls_ids = [[] for i in range(len(self.label_dict2))]\n",
    "        for i in range(len(self.label_dict2)):\n",
    "            self.slide_subtype_cls_ids[i] = np.where(self.slide_data['Subtype'] == i)[0]\n",
    "        # If not regression, store ids corresponding each tmb class at the slide level\n",
    "        if not self.regression and self.label_column in ['TMB (nonsynonymous)', 'Mutation Count']:\n",
    "            self.slide_cls_ids = [[] for i in range(len(self.label_dict))]\n",
    "            for i in range(len(self.label_dict)):\n",
    "                # self.slide_cls_ids[i] = np.where(self.slide_data[self.label_column] == list(self.label_dict.keys())[i])[0]\n",
    "                self.slide_cls_ids[i] = np.where(self.slide_data[self.label_column] == i)[0]\n",
    "        else:\n",
    "            # weighted loss 구할 때 self.slide_cls_ids 필요\n",
    "            # regression이더라도 subtype별로 weighted loss하고자 하면 필요할까봐 일단 변수 만들어 놓음\n",
    "            self.slide_cls_ids = self.slide_subtype_cls_ids\n",
    "            # print(self.slide_cls_ids)\n",
    "            \n",
    "\n",
    "    def summarize(self):\n",
    "        if self.balance[1]:\n",
    "            for i in range(len(self.label_dict2)):\n",
    "                print(f'{self.split_key} Slide-LVL; Number of samples registered in subtype class {i}: {self.slide_subtype_cls_ids[i].shape[0]}')\n",
    "\n",
    "        if self.balance[0] and not self.regression and self.label_column in ['TMB (nonsynonymous)', 'Mutation Count']:\n",
    "            for i in range(len(self.label_dict)): \n",
    "                print(f'{self.split_key} Slide-LVL; Number of samples registered in tmb class {i}: {self.slide_cls_ids[i].shape[0]}')\n",
    "\n",
    "    def getlabel(self, ids):\n",
    "        '''\n",
    "        이렇게 하면 label_column에서 가져오는데 regression에서는 int로 안바꿨으므로 문제 생김\n",
    "        regression에서는 굳이 이 함수를 부르게 되면 tmb에 대한 label이 아니라 \n",
    "        '''\n",
    "        return self.slide_data[self.label_column][ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1730, -0.5198, -0.7209, -0.5456,  0.5410, -1.3605,  1.4543,  0.6625,\n",
      "          0.1969, -0.6286],\n",
      "        [ 1.0279,  0.7707,  0.4302, -0.4608,  0.5533, -1.4768,  1.3008, -0.6745,\n",
      "          0.5119,  0.2290],\n",
      "        [-1.8112, -0.7118,  1.2214, -0.2534, -0.2556,  0.9658, -0.5006, -0.2799,\n",
      "          0.4337, -0.0267],\n",
      "        [ 0.8516, -0.1119,  0.7187,  0.2937,  1.9620, -1.1525, -0.2221, -0.0307,\n",
      "          1.7035,  0.6476],\n",
      "        [ 1.5664,  0.8207, -2.1738, -0.3423, -0.1067, -0.6570,  0.1420,  0.1107,\n",
      "          0.6497, -0.3075]])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "tensor(2.3189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.randn((5,10))\n",
    "b = torch.arange(len(a))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.type())\n",
    "print(b.type())\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(loss(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-4):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
