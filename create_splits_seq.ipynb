{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMB split 만들때 subtyping까지 고려해서 만들도록\n",
    "이미 subtype 별로 TMB high, low balance가 맞아서 별로 효과 없었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label column: label\n",
      "label dictionary: {'TMB_low': 0, 'TMB_high': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    686\n",
      "1    255\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 686\n",
      "Slide-LVL; Number of samples registered in class 0: 686\n",
      "Patient-LVL; Number of samples registered in class 1: 255\n",
      "Slide-LVL; Number of samples registered in class 1: 255\n",
      "Oncotree Code  LUAD  LUSC\n",
      "label                    \n",
      "0               346   340\n",
      "1               141   114\n",
      "[720 704 330 140 193 384 154  74 288 321 445 884 216 761 917 386 185 759\n",
      " 512 647 542  70 419 341 898 474 615 835  55 176 502 678 767 196 849 847\n",
      " 563 817 276 780 715 489 923 166 722 101 107 183 586 723 494  81 646 313\n",
      " 706 443  38 481 744 226 218 462 124 737 406  96  35 558 246 551 143 894\n",
      " 820 475 245 679 673 868 559 731 688 550 129 567 499 802 786  37 581 232\n",
      " 867 181 750 331 300 464   3 333 614 104 540 578 636 492 661 574 393 253\n",
      " 685 160 467 272 472 168 319 468 602 539 746 860 146 580 572 911 486 590\n",
      " 361  25 415  46 173 698  57 430 793 537 938 764  99 498 690  98  92 708\n",
      " 785 274 165  23 291 660 662 798 192  17 105 587 777 432 575 854 929 832\n",
      " 703 306 394 585 833 651 345  87 675 895 122 686 837 395 269 280 401 149\n",
      " 178 100 656 933 626 301 866 145 201 619 709 707 775 340 297 366 605  12\n",
      " 342 174 861 757 388 568 452 326 150 199 303 629 270 292 907 458 807 157\n",
      " 812 501 220 286 103 664 637 102 554 275 212 372 791 591 374 672 273 569\n",
      " 323 446 522  91 112 289  95 127 377 620  11 411 804 517  56 889 515 378\n",
      " 872 497 350 281 593 364 385 504 325 577 247  79 412 598 915 285 479 840\n",
      " 496  39 735 312 726 627  15 843 745 308 913 910  40 630 237 875 773 106\n",
      " 442 666  28 677  24 435 869 896  32 800  21 676 918 373 420 500 524  71\n",
      " 120 604 523  83 202 241 850 152 454 878 680 405  61 121 116 433 927 437\n",
      " 634 403 261 128 573 771  48 197 302 696  86 139 180 782 600 398 810 671\n",
      " 182  33 383 864 931 904 546 279   1 633 349 549 682  90 215 109 296 858\n",
      " 874 613 526 724 184 663 469 380 495 513 431 271 208 751 691 912 650 857\n",
      " 244 538 447 170 222 596 460 134 381  41 264 610 175  16 429 171  14 885\n",
      " 234 177 363 937 483  29  94 339 370 282 583 426 114 533 318 552 789 441\n",
      " 142 919 179 695 290 392 659 805 625 480 734 632 144 375 320 491 774 390\n",
      " 638  52 309 338 845  58  20 881 238 548 566 172 859 628 455 778 582 877\n",
      " 229   2 739 147 307 217 159 556  31 584 293 654 359 700 295 599 242 815\n",
      " 806 167 555 332 667  76 640 249 453 532 753 346 732 544 825 754 327 256\n",
      "  80 547 259 902 348 887 118  60  93 507 882 642 466 799 126 564 141  72\n",
      "  42 250 352 844  34 231 451 758  77 391 351 310 334 277 886 705 236 818\n",
      " 906  53 645 421 553 748 865 195 315 790 648 639 260 813 560 476 505 727\n",
      " 932  85 111 113 608 641  97 914 439 343 612 511 653 317 916 826 449 336\n",
      " 618  54 562 108 873 493  84 936 643 579 821 478 230 743 438 444 925 408\n",
      " 769 921 766  44 733  30 788 335 409 617 485 365 623 214  36 728 337 248\n",
      " 909 926 721 829 457 389 893 920 699 828 266 133 891 711 876 652 543 871\n",
      " 490  49 477 693 221 404 211  13 265 908 422 137 851 200 729 928 509 740\n",
      " 298 278 368 239 862 903 328 402 607 225 233 557 514 506 702 125 570 521\n",
      " 410 252 839 905 206 624 665 781 658]\n",
      "[ 45 779 684  19 710 765 204 131 601 525  88 354 770 267 223 369  10 516\n",
      " 674 594 484 416 205 848  43 294 762   9 853 268 304 243   5 213  73 329\n",
      "  59  89 701 219 510 879 262 156 841 459 822 434 883 763 852 376 397 595\n",
      "  65 576 287 597  50 387 427 856 518 210 135 622 836 545 324 935 503 283\n",
      " 257 655  67 736 901 827 768 305 669 463 712 846 772 830  64 187 814 396\n",
      " 465 719 448 797 694]\n",
      "[687 730 535 834 611 609 186 531 362 530 344 809 487 400   7 880 792 488\n",
      "   0 561 760 407 714 379  18 725 117 589 899 718 163 692 316 823 838 425\n",
      " 353  75  82 742 644  51 716 418 190 755 138 713 697 162 360 164 819 776\n",
      " 635 888 136 413 795 240 194 471 130 681 258 224 461 657 796 571 592 123\n",
      "  27 689 565 284   6 811 855 924 311 473  26 603 668 536 787 382 371 606\n",
      " 440 541 842 450  62 870 191 423  22 890 528 235 534 756  68 148 520 110\n",
      " 616 717 801 588 228 741 824 508 670   8 749 794 119 115 930 188 356 189\n",
      " 169 752 892 355 900 399 529 621 358 808 831  47  66 922   4 897 255  78\n",
      " 863 203 436 470 132 414 151 428 784 161 803 153 367 816 314 783 251  63\n",
      " 519 939 207 424 417 263 649 527 683 934 482 209  69 158 227 456 738 322\n",
      " 940 747 254 299 347 198 631 155 357]\n",
      "\n",
      "number of training samples: 657\n",
      "number of samples in cls 0: 479\n",
      "number of samples in cls 1: 178\n",
      "\n",
      "number of val samples: 95\n",
      "number of samples in cls 0: 69\n",
      "number of samples in cls 1: 26\n",
      "\n",
      "number of test samples: 189\n",
      "number of samples in cls 0: 138\n",
      "number of samples in cls 1: 51\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import os\n",
    "# os.chdir('/shared/j.jang/pathai/CLAM')\n",
    "import pandas as pd\n",
    "from datasets.dataset_generic import Generic_WSI_Classification_Dataset, Generic_MIL_Dataset, save_splits\n",
    "import numpy as np\n",
    "# from utils.utils import generate_split, nth\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "csv_path = '/shared/js.yun/CLAM/dataset_csv/TCGA-lung-LUAD+LUSC-TMB-pan_cancer-323_HIPT_only.csv'\n",
    "# split_dir = '/shared/js.yun/data/CLAM_data/TCGA-lung-luad+lusc-TMB-323-HIPT-only-splits/'\n",
    "split_dir = '/shared/js.yun/data/CLAM_data/test/'\n",
    "label_dict = {\"TMB_low\":0, \"TMB_high\":1}\n",
    "\n",
    "task = 'task_1_tumor_vs_normal'\n",
    "seed = 1\n",
    "label_frac = 1.0\n",
    "val_frac = 0.1\n",
    "test_frac = 0.2\n",
    "k = 1\n",
    "n_class = 2\n",
    "\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "class TMB_Dataset(Generic_WSI_Classification_Dataset):\n",
    "    def __init__(self,\n",
    "        subtype_head = 'Oncotree Code', \n",
    "        **kwargs):\n",
    "        super(TMB_Dataset, self).__init__(**kwargs)\n",
    "\n",
    "        self.subtype_col = self.slide_data[subtype_head]\n",
    "        self.subtype_classes = np.unique(self.subtype_col)\n",
    "        self.tmb_count_group_by_subtype = self.slide_data.groupby('label')[self.subtype_col.name].value_counts().unstack().fillna(0)\n",
    "        print(self.tmb_count_group_by_subtype)\n",
    "        # print(self.subtype_col.value_counts())\n",
    "        # print(self.slide_data.groupby('label')[self.subtype_col.name].value_counts().unstack().fillna(0))\n",
    "        \n",
    "        \n",
    "        num_slides_cls = np.array([len(cls_ids) for cls_ids in dataset.patient_cls_ids])\n",
    "        val_num = np.round(num_slides_cls * val_frac).astype(int)\n",
    "        test_num = np.round(num_slides_cls * test_frac).astype(int)\n",
    "        dataset.create_splits(k = k, val_num = val_num, test_num = test_num)\n",
    "\n",
    "        self.split_gen = self.stratified_split(0.7, 0.1, 0.2)\n",
    "\n",
    "    def create_splits(self, k = 3, val_num = (25, 25), test_num = (40, 40), label_frac = 1.0, custom_test_ids = None):\n",
    "        '''\n",
    "        얘는 딱히 고칠거 없음\n",
    "        self.split_gen에다가 generate_split 붙여주는 역할\n",
    "        '''\n",
    "        settings = {\n",
    "                    'n_splits' : k, \n",
    "                    'val_num' : val_num, \n",
    "                    'test_num': test_num,\n",
    "                    'label_frac': label_frac,\n",
    "                    'seed': self.seed,\n",
    "                    'custom_test_ids': custom_test_ids\n",
    "                    }\n",
    "\n",
    "        if self.patient_strat:\n",
    "            settings.update({'cls_ids' : self.patient_cls_ids, 'samples': len(self.patient_data['case_id'])})\n",
    "        else:\n",
    "            settings.update({'cls_ids' : self.slide_cls_ids, 'samples': len(self.slide_data)})\n",
    "\n",
    "        # self.split_gen = self.generate_split(**settings)\n",
    "\n",
    "    def stratified_split(self, a, b, c):\n",
    "        # Create a combined stratification column\n",
    "        self.slide_data['strata'] = self.slide_data['label'].astype(str) + \"_\" + self.subtype_col.astype(str)\n",
    "\n",
    "        # Calculate the number of splits based on the provided ratios\n",
    "        test_size = c / (a + b + c)\n",
    "        val_size = b / (a + b)  # Adjusted this line\n",
    "\n",
    "        # Create the stratified splitter for train+val and test\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "        for train_val_index, test_index in sss.split(self.slide_data, self.slide_data['strata']):\n",
    "            pass\n",
    "\n",
    "        # Create the stratified splitter for train and val within train+val\n",
    "        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=42)\n",
    "        for train_index, val_index in sss_val.split(self.slide_data.iloc[train_val_index], self.slide_data.iloc[train_val_index]['strata']):\n",
    "            train_index = train_val_index[train_index]  # Adjust indices to original dataframe\n",
    "            val_index = train_val_index[val_index]      # Adjust indices to original dataframe\n",
    "\n",
    "        # Drop the strata column\n",
    "        self.slide_data.drop(columns=['strata'], inplace=True)\n",
    "\n",
    "        self.train_ids, self.val_ids, self.test_ids = train_index, val_index, test_index\n",
    "        \n",
    "        print(self.train_ids)\n",
    "        print(self.val_ids)\n",
    "        print(self.test_ids)\n",
    "\n",
    "        # return train_index, val_index, test_index\n",
    "\n",
    "\n",
    "    # def generate_split(cls_ids, val_num, test_num, samples, n_splits = 5,\n",
    "    #     seed = 7, label_frac = 1.0, custom_test_ids = None):\n",
    "    #     '''\n",
    "    #     호출할때마다 (sampled_train_ids, all_val_ids, all_test_ids) yield\n",
    "    #     '''\n",
    "    #     indices = np.arange(samples).astype(int)\n",
    "        \n",
    "    #     if custom_test_ids is not None:\n",
    "    #         indices = np.setdiff1d(indices, custom_test_ids)\n",
    "\n",
    "    #     np.random.seed(seed)\n",
    "    #     for i in range(n_splits):\n",
    "    #         all_val_ids = []\n",
    "    #         all_test_ids = []\n",
    "    #         sampled_train_ids = []\n",
    "            \n",
    "    #         if custom_test_ids is not None: # pre-built test split, do not need to sample\n",
    "    #             all_test_ids.extend(custom_test_ids)\n",
    "\n",
    "    #         for c in range(len(val_num)):\n",
    "    #             possible_indices = np.intersect1d(cls_ids[c], indices) #all indices of this class\n",
    "    #             val_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids\n",
    "\n",
    "    #             remaining_ids = np.setdiff1d(possible_indices, val_ids) #indices of this class left after validation\n",
    "    #             all_val_ids.extend(val_ids)\n",
    "\n",
    "    #             if custom_test_ids is None: # sample test split\n",
    "\n",
    "    #                 test_ids = np.random.choice(remaining_ids, test_num[c], replace = False)\n",
    "    #                 remaining_ids = np.setdiff1d(remaining_ids, test_ids)\n",
    "    #                 all_test_ids.extend(test_ids)\n",
    "\n",
    "    #             if label_frac == 1:\n",
    "    #                 sampled_train_ids.extend(remaining_ids)\n",
    "                \n",
    "    #             else:\n",
    "    #                 sample_num  = math.ceil(len(remaining_ids) * label_frac)\n",
    "    #                 slice_ids = np.arange(sample_num)\n",
    "    #                 sampled_train_ids.extend(remaining_ids[slice_ids])\n",
    "\n",
    "    #         yield sampled_train_ids, all_val_ids, all_test_ids\n",
    "\n",
    "if task == 'task_1_tumor_vs_normal':\n",
    "    dataset = TMB_Dataset(subtype_head = 'Oncotree Code',\n",
    "                            csv_path = csv_path,\n",
    "                            shuffle = False, \n",
    "                            seed = seed, \n",
    "                            print_info = True,\n",
    "                            label_dict = label_dict,\n",
    "                            patient_strat=True,\n",
    "                            ignore=[])\n",
    "elif task == 'task_2_tumor_subtyping':\n",
    "    dataset = Generic_WSI_Classification_Dataset(csv_path = csv_path,\n",
    "                            shuffle = False, \n",
    "                            seed = seed, \n",
    "                            print_info = True,\n",
    "                            label_dict = label_dict,\n",
    "                            patient_strat= True,\n",
    "                            patient_voting='maj',\n",
    "                            ignore=[])\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# train_data, val_data, test_data = dataset.stratified_split(0.6, 0.2, 0.2)\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# print(test_data)\n",
    "\n",
    "\n",
    "# num_slides_cls = np.array([len(cls_ids) for cls_ids in dataset.patient_cls_ids])\n",
    "# val_num = np.round(num_slides_cls * val_frac).astype(int)\n",
    "# test_num = np.round(num_slides_cls * test_frac).astype(int)\n",
    "\n",
    "\n",
    "descriptor_df = dataset.test_split_gen(return_descriptor=True)\n",
    "splits = dataset.return_splits(from_id=True)\n",
    "save_splits(splits, ['train', 'val', 'test'], os.path.join(split_dir, 'splits_{}.csv'.format(0)))\n",
    "save_splits(splits, ['train', 'val', 'test'], os.path.join(split_dir, 'splits_{}_bool.csv'.format(0)), boolean_style=True)\n",
    "descriptor_df.to_csv(os.path.join(split_dir, 'splits_{}_descriptor.csv'.format(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<datasets.dataset_generic.Generic_WSI_Classification_Dataset object at 0x7f624c2a7b10>\n"
     ]
    }
   ],
   "source": [
    "if label_frac > 0:\n",
    "    label_fracs = [label_frac]\n",
    "else:\n",
    "    label_fracs = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for lf in label_fracs:\n",
    "    split_dir = split_dir + str(task) + '_{}'.format(int(lf * 100))\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    dataset.create_splits(k = k, val_num = val_num, test_num = test_num, label_frac=lf)\n",
    "    for i in range(k):\n",
    "        dataset.set_splits()\n",
    "        descriptor_df = dataset.test_split_gen(return_descriptor=True)\n",
    "        splits = dataset.return_splits(from_id=True)\n",
    "        save_splits(splits, ['train', 'val', 'test'], os.path.join(split_dir, 'splits_{}.csv'.format(i)))\n",
    "        save_splits(splits, ['train', 'val', 'test'], os.path.join(split_dir, 'splits_{}_bool.csv'.format(i)), boolean_style=True)\n",
    "        descriptor_df.to_csv(os.path.join(split_dir, 'splits_{}_descriptor.csv'.format(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밑에는 test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Child\n"
     ]
    }
   ],
   "source": [
    "class Parent:\n",
    "    def say_hello(self):\n",
    "        print(\"Hello from Parent\")\n",
    "\n",
    "class Child(Parent):\n",
    "    def say_hello(self):\n",
    "        super().say_hello()  # 부모 클래스의 메서드 호출\n",
    "    def say_hello(self):\n",
    "        print(\"Hello from Child\")\n",
    "\n",
    "# 객체 생성\n",
    "child_obj = Child()\n",
    "\n",
    "# 메서드 호출\n",
    "child_obj.say_hello()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LUSC', 'LUAD']\n",
      "['LUSC', 'LUAD']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "label_dict = {'LUSC':0, 'LUAD':1}\n",
    "num_classes = 2\n",
    "\n",
    "index = [list(label_dict.keys())[list(label_dict.values()).index(i)] for i in range(num_classes)]\n",
    "print(index)\n",
    "print(list(label_dict.keys()))\n",
    "print(list(label_dict.values()).index(1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
